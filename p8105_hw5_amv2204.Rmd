---
title: "Homework 5"
author: "Ashwini Varghese"
date: "2022-11-16"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1:



## Problem 2:

```{r}
homicides = read.csv("./homicide-data.csv")

```

The raw dataset `homicides` contains `r ncol(homicides)` variables and `r nrow(homicides)` observations. It describes information on homicides from 50 large US cities. Variables of interest include the case ID, the date of the incident, information on the victim (name, race, age and sex), information on the location of the homicide, and the disposition of the case. 

We will create a new variable `city_state`.

```{r}
new_homi <- homicides %>%
  unite("city_state", city:state, sep = ", ", remove = FALSE)
```

We will first look at the total number of homicides in a city and then at the number of homicides that are unsolved in a city.

```{r}
new_homi %>% 
  group_by(city) %>%
  summarize(count = n()) %>%
  knitr::kable(digits = 1)

new_homi %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  group_by(city) %>%
  summarize(count = n()) %>%
  knitr::kable(digits = 1)
```

`prop.test`

```{r}
new_homi %>%
  filter(city_state == "Baltimore, MD") %>%
  summarize(count = n())

new_homi %>%
  filter(city_state == "Baltimore, MD") %>%
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  summarize(count = n())

```

From the above code, we see that in the city of Baltimore, MD, there were a total of 2827 homicides and of those homicides, 1825 were unsolved. We can use `prop.test` to estimate the proportion of homicides that are unsolved and its confidence interval.

```{r}
Balt_test <- prop.test(1825, 2827) %>%
  broom::tidy() %>%
  select(estimate, starts_with("conf"))
```

The proportion of homicides that are unsolved is 0.646 with a confidence interval of 0.628 and 0.663.

Now we will create a function to do this for all the cities in the dataset:

```{r}
homi_nest =
  new_homi %>% 
  relocate(city_state) %>% 
  nest(data = uid:disposition)

prop = function(df) {

  data_result = df %>% 
    summarize(total = n(),
              unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))
    
  prop_results = 
    prop.test(x = data_result %>% pull(unsolved), 
              n = data_result %>% pull(total))

  return(prop_results)
}


final <- homi_nest %>% 
  mutate(results = map(data, prop),
         estimates = map(results, broom::tidy)) %>% 
  select(city_state, estimates) %>% 
  unnest(estimates) %>%
  select(city_state, estimate, starts_with("conf"))


final %>% 
  filter(city_state != "Tulsa, AL") %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
    geom_line() +
    geom_point()+
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width=.2,
                 position=position_dodge(0.05))+
    theme(axis.text.x = element_text(angle = 90))

```

The above plot shows the proportion of unsolved homicides in each city with its corresponding confidence interval. There was an error in the data where Tulsa, AL was incorrected entered for Tulsa, OK. Instead of completely removing it from the dataset, I filtered it out of the plot.


## Problem 3:


```{r}
#rnorm(n = 30, mean = 0, sd = 5)

#rerun(5000, rnorm(n = 30, mean = 0, sd = 5))

sims = function(mu) {
  
  tibble_df = 
    tibble(x = rnorm(n = 30, mean = mu, sd = 5))
    t.test(tibble_df) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}

just_zero = 
expand.grid(mu = 0, run = 1:50) %>% 
  mutate(value = map(mu, sims)) %>%
  unnest(value)


```

```{r}
all_sims = 
expand.grid(mu = c(1:6), run = 1:50) %>% 
  mutate(value = map(mu, sims)) %>%
  unnest(value)
```


```{r}
final_set <- 
  all_sims %>%
  mutate(conclusion = if_else(p.value > 0.05, "Do not reject null", "Reject null")) %>%
  group_by(mu, conclusion) %>%
  summarise(n = n()) %>% 
  mutate(power = n/sum(n)) %>%
  filter(conclusion == "Reject null") %>% 
  select(mu, power)
  
power_plot =
  final_set %>%   
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Mu by Power",
    x = "Mu",
    y = "Power"
  ) +
    scale_x_continuous(
    breaks = c(1, 2, 3, 4, 5, 6))

power_plot

```


```{r}
avg_mu <-   
  all_sims %>%
  group_by(mu) %>%
  summarize(avg_mu = mean(estimate)) %>%
  mutate(data = "Null not rejected")

plot2 = 
  avg_mu %>%
  ggplot(aes(x = mu, y = avg_mu)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Mu by Average Mu",
    x = "Mu",
    y = "Average of Mu"
  ) +
    scale_x_continuous(
    breaks = c(1, 2, 3, 4, 5, 6))

plot2


only_rejections <- 
  all_sims %>%
  mutate(conclusion = if_else(p.value > 0.05, "Do not reject null", "Reject null")) %>% 
  filter(conclusion == "Reject null") %>% 
  group_by(mu) %>%
  summarize(avg_mu = mean(estimate)) %>%
  mutate(data = "Null rejected")


plot3 <-   
  only_rejections %>%
  ggplot(aes(x = mu, y = avg_mu)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Mu by Average Mu for Rejected Nulls",
    x = "Mu",
    y = "Average of Mu"
  ) +
    scale_x_continuous(
    breaks = c(1, 2, 3, 4, 5, 6))

plot3

```


As we can see from the plot, as the true mu gets higher, the average mu of the tests where the null is rejected gets closer to the true mu as opposed to when the mu is smaller.



